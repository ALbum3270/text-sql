"""
Text-to-SQL æ¸…æ´ç‰ˆæœ¬ - å»é™¤æ‰€æœ‰ç¡¬ç¼–ç è¡¥ä¸

æ ¸å¿ƒç†å¿µï¼š
1. æœ¬åœ°ä¸åšè¯­ä¹‰ç­›é€‰ï¼Œåªåšå®¢è§‚éªŒè¯
2. æ‰€æœ‰è¯­ä¹‰å†³ç­–äº¤ç»™LLMï¼ˆPlannerå’ŒGeneratorï¼‰
3. ç§»é™¤æ‰€æœ‰é—®é¢˜ç‰¹å®šçš„è¡¥ä¸å’Œç¡¬ç¼–ç é€»è¾‘
"""

import os
import sys
import re
import time
import json
import random
from typing import List, Dict, Any, Tuple, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed

import pymysql
import sqlglot
import sqlglot.expressions as exp
from dotenv import load_dotenv

from xiyan_client import call_xiyan
from sql_guard import validate_and_rewrite, SQLValidationError

DEFAULT_MAX_LIMIT = 200

from llm_planner import llm_plan, PlanV1
from llm_generator import llm_generate_sql, make_safety_contract
# (clean æµç¨‹æœªä½¿ç”¨ä»¥ä¸‹æ¨¡å—ï¼Œç§»é™¤ä¾èµ–)
from validation_engine import validate_and_select_best


def load_schema(path: str) -> Dict[str, Any]:
    """åŠ è½½M-Schema"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def filter_m_schema(m_schema: Dict[str, Any], table_names: List[str]) -> Dict[str, Any]:
    """æ ¹æ®è¡¨ååˆ—è¡¨è¿‡æ»¤M-Schema"""
    filtered = {"tables": []}
    for table in m_schema.get("tables", []):
        if table.get("name") in table_names:
            filtered["tables"].append(table)
    return filtered


def tokenize(text: str) -> List[str]:
    """æ”¹è¿›åˆ†è¯ï¼Œæå–ä¸­è‹±æ–‡ã€æ•°å­—ï¼ŒåŒ…å«å…³é”®è¯æå–"""
    import re
    
    # åŸºç¡€åˆ†è¯ï¼šæå–è‹±æ–‡å•è¯ã€æ•°å­—
    basic_tokens = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*|\d+', text.lower())
    
    # ä¸­æ–‡å…³é”®è¯æå–ï¼šé¢„å®šä¹‰å¸¸è§æ•°æ®åº“ç›¸å…³è¯æ±‡
    chinese_keywords = [
        "å¨èƒ", "åŸŸå", "æ¶æ„", "é»‘åå•", "åœ¨çº¿", "ç¦»çº¿", "ç»ˆç«¯", "èŠ‚ç‚¹", "çŠ¶æ€",
        "è¿æ¥", "æƒ…å†µ", "ç»Ÿè®¡", "è®°å½•", "æ•°æ®", "æ–‡ä»¶", "è¿›ç¨‹", "ç«¯å£", "æ¼æ´",
        "ç—…æ¯’", "å®‰å…¨", "é£é™©", "å‘Šè­¦", "æ—¥å¿—", "æ—¶é—´", "ä»Šå¤©", "æ˜¨å¤©", "è¶‹åŠ¿",
        "è®¡æ•°", "æ€»æ•°", "åˆ†å¸ƒ", "æŒ‰", "æŸ¥è¯¢", "æ£€ç´¢", "æœç´¢", "åˆ—è¡¨", "è¯¦æƒ…",
        "ç”¨æˆ·", "è´¦å·", "å¯†ç ", "å¼±å£ä»¤", "æ”»å‡»", "é˜²æŠ¤", "ç›‘æ§", "åˆ†æ",
        "èµ„äº§", "è®¾å¤‡", "ä¸»æœº", "æœåŠ¡å™¨", "ç½‘ç»œ", "æµé‡", "å¼‚å¸¸", "äº‹ä»¶"
    ]
    
    # åœ¨ä¸­æ–‡æ–‡æœ¬ä¸­æŸ¥æ‰¾å…³é”®è¯
    chinese_tokens = []
    for keyword in chinese_keywords:
        if keyword in text:
            chinese_tokens.append(keyword)
    
    # åˆå¹¶æ‰€æœ‰token
    all_tokens = basic_tokens + chinese_tokens
    
    # å»é‡å¹¶è¿‡æ»¤
    unique_tokens = list(set([t for t in all_tokens if t and len(t) > 0]))
    
    return unique_tokens


def score_table_simple(table: Dict[str, Any], tokens: List[str]) -> float:
    """ç®€å•çš„è¡¨è¯„åˆ†ï¼ŒåŸºäºå…³é”®è¯åŒ¹é…ï¼Œå¼ºåŒ–è¡¨å/åˆ—åç²¾ç¡®åŒ¹é…æƒé‡"""
    name = str(table.get("name", "")).lower()
    score = 0.0
    
    # ğŸ¯ ä¸­è‹±æ–‡è¯­ä¹‰æ˜ å°„ï¼šè®©ä¸­æ–‡è¯æ±‡èƒ½åŒ¹é…è‹±æ–‡è¡¨å
    semantic_mapping = {
        "å¨èƒ": ["threat", "malicious", "risk"],
        "åŸŸå": ["domain", "url", "dns"], 
        "æ¶æ„": ["malicious", "threat", "bad"],
        "é»‘åå•": ["blacklist", "block", "deny"],
        "åœ¨çº¿": ["online", "connected", "active", "statistics"],
        "ç¦»çº¿": ["offline", "disconnected", "inactive", "statistics"],
        "ç»ˆç«¯": ["node", "endpoint", "terminal", "machine"],
        "èŠ‚ç‚¹": ["node", "endpoint", "machine"],
        "çŠ¶æ€": ["status", "state", "statistics"],
        "è¿æ¥": ["connect", "connection", "link", "statistics"],
        "æƒ…å†µ": ["statistics", "status", "state", "summary"],  # æ–°å¢
        "æ€ä¹ˆæ ·": ["statistics", "summary", "status"],  # æ–°å¢  
        "ç»Ÿè®¡": ["statistics", "stat", "count", "summary"],
        "è®°å½•": ["record", "log", "entry"],
        "æ–‡ä»¶": ["file", "document"],
        "è¿›ç¨‹": ["process", "proc"],
        "ç«¯å£": ["port"],
        "æ¼æ´": ["vulnerability", "vuln", "cve"],
        "ç—…æ¯’": ["virus", "malware"],
        "ç”¨æˆ·": ["user", "account"],
        "å¯†ç ": ["password", "pwd"],
        "å¼±å£ä»¤": ["weak", "password"],
        "ç›‘æ§": ["monitor", "watch"],
        "åˆ†æ": ["analysis", "analyze"],
        "è¶‹åŠ¿": ["trend", "statistics", "time"],  # æ–°å¢
        "æ€»æ•°": ["count", "total", "summary"],  # æ–°å¢
        "åˆ†å¸ƒ": ["distribution", "group", "statistics"]  # æ–°å¢
    }
    
    # æ‰©å±•tokensï¼šæ·»åŠ è¯­ä¹‰æ˜ å°„çš„è‹±æ–‡è¯æ±‡
    extended_tokens = tokens.copy()
    for token in tokens:
        if token in semantic_mapping:
            extended_tokens.extend(semantic_mapping[token])
    
    # ğŸ¯ è¡¨åå®Œå…¨åŒ¹é…ï¼šæœ€é«˜æƒé‡
    if name and name in set(extended_tokens):
        score += 10.0  # å¤§å¹…æå‡å®Œå…¨åŒ¹é…æƒé‡
    
    # ğŸ¯ è¡¨ååˆ†è¯ç²¾ç¡®åŒ¹é…ï¼šå¦‚ "threat" ç²¾ç¡®åŒ¹é… "threat_domain_static"
    table_parts = name.split('_') if name else []
    for part in table_parts:
        if part and part in extended_tokens:
            score += 5.0  # æå‡è¡¨åç»„æˆéƒ¨åˆ†åŒ¹é…æƒé‡
    
    # ğŸ¯ è¡¨ååŒ…å«åŒ¹é…ï¼šéƒ¨åˆ†åŒ…å«å…³ç³»
    for tk in extended_tokens:
        if tk and len(tk) > 2 and tk in name:  # é¿å…å¤ªçŸ­çš„tokenå¹²æ‰°
            score += 1.0
    
    # ğŸ¯ å¤štokenè¯­ä¹‰åŒ¹é…ï¼šå¦‚ ["å¨èƒ","åŸŸå"] é€šè¿‡æ˜ å°„åŒ¹é… threat_domain
    # æ£€æŸ¥æ˜ å°„åçš„tokenåœ¨è¡¨åä¸­çš„åŒ¹é…æƒ…å†µ
    semantic_matches = 0
    for token in tokens:
        if token in semantic_mapping:
            mapped_words = semantic_mapping[token]
            for mapped in mapped_words:
                if mapped in name:
                    semantic_matches += 1
                    break  # æ¯ä¸ªåŸè¯åªè®¡ç®—ä¸€æ¬¡åŒ¹é…
    
    if semantic_matches >= 2:
        score += 8.0  # å¤šä¸ªè¯­ä¹‰åŒ¹é…ï¼šé«˜æƒé‡
    elif semantic_matches >= 1:
        score += 4.0  # å•ä¸ªè¯­ä¹‰åŒ¹é…ï¼šä¸­ç­‰æƒé‡
    
    # ğŸ¯ ç‰¹æ®ŠåŠ æƒï¼šç»Ÿè®¡æ€§æŸ¥è¯¢ä¼˜å…ˆåŒ¹é…statisticsè¡¨
    statistical_indicators = ["æƒ…å†µ", "æ€ä¹ˆæ ·", "ç»Ÿè®¡", "æ€»æ•°", "åˆ†å¸ƒ", "è¶‹åŠ¿"]
    is_statistical_query = any(indicator in tokens for indicator in statistical_indicators)
    
    if is_statistical_query and "statistics" in name:
        score += 20.0  # ç»Ÿè®¡æ€§æŸ¥è¯¢å¤§å¹…åŠ æƒstatisticsè¡¨
        print(f"ğŸ¯ ç»Ÿè®¡æ€§æŸ¥è¯¢æ£€æµ‹ï¼šä¸º {name} è¡¨å¢åŠ ç»Ÿè®¡åŠ æƒ")
    
    # ğŸ¯ ç‰¹æ®ŠåŠ æƒï¼šå¨èƒç›¸å…³æŸ¥è¯¢ä¼˜å…ˆåŒ¹é…ä¸“ä¸šå¨èƒè¡¨
    threat_indicators = ["å¨èƒ", "æ¶æ„", "é»‘åå•"]
    is_threat_query = any(indicator in tokens for indicator in threat_indicators)
    
    if is_threat_query and any(word in name for word in ["threat", "malicious", "blacklist"]):
        score += 10.0  # å¨èƒæŸ¥è¯¢åŠ æƒä¸“ä¸šå¨èƒè¡¨
    
    # ğŸ¯ åˆ—ååŒ¹é…ä¼˜åŒ–  
    cols = table.get("columns", [])
    col_names = [str(c.get("name", "")).lower() for c in cols]
    
    # é€šç”¨åˆ—é™æƒï¼šé¿å…å¸¸è§åˆ—åå¹²æ‰°å¬å›
    COMMON_COLUMNS = {"id", "name", "value", "key", "type", "status", "time", "date", 
                     "create_time", "update_time", "start_time", "end_time", "level"}
    
    for cn in col_names:
        lc = cn
        for tk in extended_tokens:  # ä½¿ç”¨æ‰©å±•åçš„tokens
            if tk and tk in lc:
                if tk == lc:  # ğŸ¯ åˆ—åå®Œå…¨åŒ¹é…ï¼šé«˜æƒé‡
                    if lc not in COMMON_COLUMNS:
                        score += 2.0  # éé€šç”¨åˆ—å®Œå…¨åŒ¹é…
                    else:
                        score += 0.1  # é€šç”¨åˆ—å®Œå…¨åŒ¹é…ï¼šå¾ˆä½æƒé‡
                elif lc not in COMMON_COLUMNS:  # ğŸ¯ éé€šç”¨åˆ—éƒ¨åˆ†åŒ¹é…
                    score += 0.5
                else:  # ğŸ¯ é€šç”¨åˆ—éƒ¨åˆ†åŒ¹é…ï¼šå¤§å¹…é™æƒ
                    score -= 0.9  # å®é™…ä¸Šæ˜¯é™ä½åˆ†æ•°ï¼Œé¿å…å¹²æ‰°
    
    # ğŸ¯ è¡¨åç²¾ç¡®å‘½ä¸­é¢å¤–åŠ æƒï¼šç¡®ä¿ç²¾ç¡®åŒ¹é…è¡¨åæ’åœ¨æœ€å‰
    for tk in extended_tokens:
        if tk and tk == name:
            score += 15.0  # è¶…é«˜æƒé‡ç¡®ä¿æ’åºä¼˜å…ˆ
    
    return max(0.0, score)  # ç¡®ä¿åˆ†æ•°éè´Ÿ


def auto_select_tables(m_schema: Dict[str, Any], tokens: List[str], topk: int = 8) -> Tuple[Dict[str, Any], List[Tuple[str, float]]]:
    """åŸºäºå…³é”®è¯çš„è¡¨é€‰æ‹©ï¼ˆçº¯å®¢è§‚åŒ¹é…ï¼‰ï¼Œæ”¯æŒåŠ¨æ€topk"""
    tables = m_schema.get("tables", [])
    scored = []
    
    for t in tables:
        s = score_table_simple(t, tokens)
        scored.append((t.get("name"), s))
    
    scored.sort(key=lambda x: x[1], reverse=True)
    
    # ğŸ¯ åŠ¨æ€è°ƒæ•´topkï¼šå‘ç°ç²¾ç¡®åŒ¹é…æ—¶æ‰©å¤§å¬å›èŒƒå›´
    dynamic_topk = topk
    max_score = scored[0][1] if scored else 0
    
    # å¦‚æœæœ€é«˜åˆ†â‰¥10ï¼Œè¯´æ˜æœ‰è¡¨åç²¾ç¡®åŒ¹é…ï¼Œé€‚å½“å¢åŠ å¬å›æ•°é‡
    if max_score >= 10.0:
        dynamic_topk = min(topk + 4, len(scored))  # å¢åŠ 4ä¸ªå€™é€‰
        print(f"ğŸ¯ å‘ç°ç²¾ç¡®åŒ¹é…è¡¨å(åˆ†æ•°:{max_score:.1f})ï¼Œæ‰©å¤§å¬å›åˆ° {dynamic_topk} ä¸ªå€™é€‰")
    
    # å¦‚æœæœ‰å¤šä¸ªé«˜åˆ†è¡¨(â‰¥5åˆ†)ï¼Œä¹Ÿé€‚å½“å¢åŠ 
    high_score_count = sum(1 for _, score in scored if score >= 5.0)
    if high_score_count >= 2:
        dynamic_topk = min(topk + 2, len(scored))
        print(f"ğŸ¯ å‘ç° {high_score_count} ä¸ªé«˜åˆ†è¡¨ï¼Œæ‰©å¤§å¬å›åˆ° {dynamic_topk} ä¸ªå€™é€‰")
    
    chosen_names = [n for n, _ in scored[:max(1, dynamic_topk)]]
    filtered = filter_m_schema(m_schema, chosen_names)
    
    return filtered, scored[:max(1, dynamic_topk)]


def _select_columns_simple(effective_schema: Dict[str, Any], table_names: List[str], 
                          tokens: List[str], topk_per_table: int = 12) -> Dict[str, List[str]]:
    """ç®€å•çš„åˆ—é€‰æ‹©ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼Œæ— ç¡¬ç¼–ç åå¥½ï¼‰"""
    selected: Dict[str, List[str]] = {}
    
    for tname in table_names:
        table_entry = next((t for t in effective_schema.get("tables", []) 
                           if t.get("name") == tname), None)
        if not table_entry:
            continue
            
        cols = table_entry.get("columns", [])
        scored: List[Tuple[float, str]] = []
        
        for c in cols:
            cname = str(c.get("name", ""))
            lc = cname.lower()
            comment = str(c.get("comment", "")).lower()
            score = 0.0
            
            # çº¯å…³é”®è¯åŒ¹é…
            for tk in tokens:
                if tk and tk in lc:
                    score += 1.0
                if tk and tk in comment:
                    score += 0.3
            
            scored.append((score, cname))
        
        # ç®€å•æ’åºï¼Œå–å‰Nä¸ª
        scored.sort(key=lambda x: x[0], reverse=True)
        top = [n for _, n in scored[:max(1, topk_per_table)]]
        selected[tname] = top
    
    return selected


def load_kb_catalog() -> Dict[str, Any]:
    """åŠ è½½KBç›®å½•"""
    kb_dir = os.path.join("outputs", "kb")
    catalog_path = os.path.join(kb_dir, "kb_catalog.json")
    if os.path.exists(catalog_path):
        with open(catalog_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def extract_kb_snippet(kb_catalog: Dict[str, Any], table_names: List[str], max_chars: int = 2000) -> str:
    """æå–KBç‰‡æ®µ"""
    if not kb_catalog:
        return ""
    
    kb_dir = os.path.join("outputs", "kb")
    parts: List[str] = []
    
    for t in table_names:
        md_path = os.path.join(kb_dir, f"{t}.md")
        if os.path.exists(md_path):
            with open(md_path, "r", encoding="utf-8") as f:
                content = f.read()
                parts.append(f"## è¡¨ {t}\n{content[:1000]}")
    
    result = "\n\n".join(parts)
    return result[:max_chars] if len(result) > max_chars else result


def _build_allowed_columns_string(effective_schema: Dict[str, Any], 
                                 table_names: List[str],
                                 selected_columns_by_table: Optional[Dict[str, List[str]]] = None,
                                 trend_time_col: str = "",
                                 total_mode: bool = False,
                                 filter_columns_by_table: Optional[Dict[str, List[str]]] = None) -> str:
    """æ„å»ºå…è®¸åˆ—çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
    lines: List[str] = []
    
    for tname in table_names:
        table_entry = next((t for t in effective_schema.get("tables", []) 
                           if t.get("name") == tname), None)
        if not table_entry:
            continue
        
        if selected_columns_by_table and tname in selected_columns_by_table:
            cols = selected_columns_by_table[tname]
        else:
            cols = [c.get("name") for c in table_entry.get("columns", [])]
        
        # è¶‹åŠ¿æ¨¡å¼ç‰¹æ®Šå¤„ç†
        if trend_time_col and not total_mode:
            if trend_time_col not in cols:
                cols = [trend_time_col] + cols
        
        # è¿‡æ»¤åˆ—
        if filter_columns_by_table and tname in filter_columns_by_table:
            filter_cols = filter_columns_by_table[tname]
            cols = [c for c in cols if c not in filter_cols]
        
        if cols:
            lines.append(f"{tname}: {', '.join(cols[:15])}")  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
    
    return "\n".join(lines)


def _build_evidence(question: str, table_names: List[str], 
                   effective_schema: Dict[str, Any]) -> List[str]:
    """æ„å»ºåŸºç¡€è¯æ®ï¼ˆæ— ç¡¬ç¼–ç è§„åˆ™ï¼‰"""
    parts = []
    
    # æ—¶é—´æç¤º
    q_lower = question.lower()
    if any(k in q_lower for k in ["æœ€è¿‘", "è¿‘", "è¿‡å»", "last", "recent"]):
        parts.append("æ—¶é—´è¿‡æ»¤æç¤º: ä½¿ç”¨ DATE_SUB æˆ– INTERVAL è¯­æ³•")
    
    # JOINæç¤ºï¼ˆåŸºäºå¤–é”®ï¼‰
    fks = []
    for t in effective_schema.get("tables", []):
        if t.get("name") in table_names:
            for fk in t.get("foreign_keys", []):
                ref_table = fk.get("ref_table")
                if ref_table in table_names:
                    fks.append(f"{t.get('name')}.{fk.get('column')} = {ref_table}.{fk.get('ref_column')}")
    
    if fks:
        parts.append(f"å¯èƒ½çš„è¿æ¥: {'; '.join(fks[:3])}")
    
    return parts


def _get_mysql_conn():
    """è·å–MySQLè¿æ¥"""
    host = os.getenv("MYSQL_HOST", "127.0.0.1")
    port = int(os.getenv("MYSQL_PORT", "3306"))
    user = os.getenv("MYSQL_USER", "root")
    password = os.getenv("MYSQL_PASSWORD", "")
    database = os.getenv("MYSQL_DATABASE", "test")
    
    return pymysql.connect(
        host=host, port=port, user=user, password=password,
        database=database, charset="utf8mb4"
    )


def exec_explain_mysql(sql: str) -> List[Dict[str, Any]]:
    """æ‰§è¡ŒEXPLAIN"""
    conn = _get_mysql_conn()
    try:
        with conn.cursor() as cur:
            cur.execute(f"EXPLAIN {sql}")
            cols = [d[0] for d in cur.description]
            rows = cur.fetchall()
            return [dict(zip(cols, r)) for r in rows]
    finally:
        conn.close()


def component_score(sql: str, weights: Dict[str, float]) -> float:
    """è®¡ç®—SQLç»„ä»¶å¤æ‚åº¦åˆ†æ•°ï¼ˆè¶Šç®€å•è¶Šå¥½ï¼‰"""
    try:
        expr = sqlglot.parse_one(sql, read="mysql")
        
        # è®¡ç®—å¤æ‚åº¦
        join_count = len(list(expr.find_all(exp.Join)))
        col_count = len(list(expr.find_all(exp.Column)))
        where_count = len(list(expr.find_all(exp.Where)))
        
        # ç®€å•æ€§åˆ†æ•°ï¼ˆè¶Šç®€å•è¶Šé«˜ï¼‰
        score = 1.0
        score -= join_count * 0.1  # JOINè¶Šå¤šåˆ†æ•°è¶Šä½
        score -= (col_count - 1) * 0.02  # åˆ—è¶Šå¤šåˆ†æ•°è¶Šä½
        score += min(where_count, 1) * 0.1  # æœ‰WHEREæ¡ä»¶åŠ åˆ†
        
        return max(0.0, min(1.0, score))
    except:
        return 0.5


def _chat_once(system: str, user: str, model: str = None, temperature: float = 0.0) -> str:
    """è°ƒç”¨LLMçš„ç»Ÿä¸€æ¥å£"""
    from openai import OpenAI
    
    # ä½¿ç”¨é…ç½®çš„æ¨¡å‹
    base_url = os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
    api_key = os.getenv("QWEN_API_KEY", os.getenv("DASHSCOPE_API_KEY", ""))
    model = model or os.getenv("QWEN_MODEL", "qwen-max-latest")
    
    if not api_key:
        raise RuntimeError("æœªè®¾ç½® QWEN_API_KEY æˆ– DASHSCOPE_API_KEY")
    
    client = OpenAI(base_url=base_url, api_key=api_key)
    
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        temperature=temperature,
        max_tokens=2000
    )
    
    return response.choices[0].message.content.strip()


# ä¸»æµç¨‹å‡½æ•°
def do_ask(args):
    """ä¸»æµç¨‹ï¼šä¸¤æ¬¡LLMè°ƒç”¨æ¶æ„"""
    load_dotenv(override=True)
    
    # åŠ è½½åŸºç¡€æ•°æ®
    m_schema = load_schema(os.path.join("outputs", "m_schema.json"))
    kb_catalog = load_kb_catalog()
    dialect = "mysql"
    
    print(f"\nğŸš€ é—®é¢˜: {args.question}")
    
    # Step 0: è½»é‡å¬å›ï¼ˆåŸå§‹å€™é€‰ï¼‰
    print("\nğŸ“Š Step 0: è½»é‡å€™é€‰å¬å›...")
    tokens = tokenize(args.question)
    
    # å…³é”®è¯å¬å›
    effective_schema_kw, scored_tables_kw = auto_select_tables(m_schema, tokens, topk=8)
    semantic_tables_raw = [name for name, _ in scored_tables_kw]
    
    # è¯­ä¹‰å¬å›ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.use_semantic:
        try:
            # ä¸ä»“åº“ä¸€è‡´ï¼šä½¿ç”¨ semantic_retrieval.pyï¼ˆåŸºäº index_dir çš„ç­¾åï¼‰
            from semantic_retrieval import semantic_suggest_with_index
            sem_res = semantic_suggest_with_index(
                args.question,
                index_dir="outputs/semantic_index",
                top_tables=8,
                top_cols=24
            )
            if sem_res:
                # å…¼å®¹è¿”å› (tables, colmap) æˆ–ä»… tables
                if isinstance(sem_res, tuple):
                    sem_tables, _ = sem_res
                else:
                    sem_tables = sem_res
                # åˆå¹¶ç»“æœ
                seen = set(semantic_tables_raw)
                for name, _ in sem_tables:
                    if name not in seen:
                        semantic_tables_raw.append(name)
                        seen.add(name)
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
    
    # é™åˆ¶å€™é€‰è¡¨æ•°é‡
    semantic_tables_raw = semantic_tables_raw[:12]
    effective_schema = filter_m_schema(m_schema, semantic_tables_raw)
    
    # åŸå§‹åˆ—å¬å›
    semantic_colmap_raw = _select_columns_simple(
        effective_schema, semantic_tables_raw, tokens, topk_per_table=15
    )
    
    print(f"âœ… åŸå§‹å€™é€‰è¡¨: {semantic_tables_raw[:6]}...")
    
    # Step 1: LLMè°ƒç”¨#1 - Planner
    print("\nğŸ§  Step 1: è°ƒç”¨Plannerç”Ÿæˆæ‰§è¡Œè®¡åˆ’...")
    
    kb_snippet = extract_kb_snippet(kb_catalog, semantic_tables_raw, max_chars=2000)
    schema_clip = json.dumps(effective_schema, ensure_ascii=False)[:3000]
    
    try:
        plan = llm_plan(
            question=args.question,
            kb_hint=kb_snippet,
            schema_clip=schema_clip,
            semantic_tables=semantic_tables_raw,
            semantic_colmap=semantic_colmap_raw
        )
        print(f"âœ… Planç”Ÿæˆ: task={plan.task}, subject={plan.subject}, risk={plan.risk}")
    except Exception as e:
        print(f"âŒ Plannerå¤±è´¥: {e}")
        # å›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼
        return do_ask_traditional(args, m_schema, kb_catalog, dialect)
    
    # Step 2: åº”ç”¨Planåˆ°ä¸Šä¸‹æ–‡
    print("\nğŸ”§ Step 2: åº”ç”¨Planæ„å»ºä¸Šä¸‹æ–‡...")
    
    # åˆå¹¶ required_tablesï¼Œä½†åªä¿ç•™ Schema ä¸­çœŸå®å­˜åœ¨çš„è¡¨
    table_names_for_kb = list(semantic_tables_raw)
    global_tables = {t.get('name') for t in m_schema.get('tables', [])}
    for t in plan.required_tables:
        if t in global_tables and t not in table_names_for_kb:
            table_names_for_kb.append(t)

    # ä¾æ® MUST è°“è¯/è¿æ¥ä¸­å‡ºç°çš„è¡¨å‰ç¼€å°è¯•è¡¥å……ï¼Œä½†åŒæ ·ä»…é™ Schema å·²å­˜åœ¨çš„è¡¨
    try:
        pred_sources = (plan.required_predicates or []) + getattr(plan, 'must_predicates', [])
        join_sources = plan.required_joins or []
        inferred: set = set()
        for text in pred_sources + join_sources:
            for match in re.findall(r"\b([a-zA-Z_][a-zA-Z0-9_]*)\.", str(text)):
                if match in global_tables:
                    inferred.add(match)
        for name in sorted(inferred):
            if name not in table_names_for_kb:
                table_names_for_kb.append(name)
    except Exception:
        pass
    
    # é‡å»ºschema
    effective_schema = filter_m_schema(m_schema, table_names_for_kb)
    
    # æ ¹æ®plané‡æ’åˆ—ï¼ˆä½†ä¸åšç¡¬ç¼–ç åå¥½ï¼‰
    selected_cols = dict(semantic_colmap_raw)
    if plan.projection_priority:
        for t, cols in selected_cols.items():
            # ä¼˜å…ˆæ˜¾ç¤ºplanæŒ‡å®šçš„åˆ—
            priority_cols = [c for c in plan.projection_priority if c in cols]
            other_cols = [c for c in cols if c not in priority_cols]
            selected_cols[t] = (priority_cols + other_cols)[:12]

    # ä¸ºæ–°å¢æ¨æ–­çš„è¡¨è¡¥é½åˆ—é€‰æ‹©ï¼ˆæŒ‰å…³é”®è¯ç®€å•é€‰æ‹©ï¼‰
    try:
        missing_tables = [t for t in table_names_for_kb if t not in selected_cols]
        if missing_tables:
            auto_cols = _select_columns_simple(effective_schema, missing_tables, tokens, topk_per_table=12)
            for t in missing_tables:
                if t in auto_cols:
                    selected_cols[t] = auto_cols[t]
    except Exception:
        pass
    
    # æ„å»ºè¯æ®
    evidence_parts = _build_evidence(args.question, table_names_for_kb, effective_schema)
    
    # æ·»åŠ plançš„çº¦æŸä½œä¸ºè¯æ®
    if plan.required_predicates:
        evidence_parts.append(f"å¿…éœ€æ¡ä»¶: {' AND '.join(plan.required_predicates)}")
    if plan.required_joins:
        evidence_parts.append(f"å¿…éœ€è¿æ¥: {'; '.join(plan.required_joins)}")
    if plan.timeframe_days:
        evidence_parts.append(f"æ—¶é—´èŒƒå›´: æœ€è¿‘{plan.timeframe_days}å¤©")
    
    evidence_full = "\n".join(evidence_parts)
    
    # æ„å»ºå…è®¸åˆ—ï¼ˆåœ¨ä¼ å…¥Generatorå‰ï¼Œç¡®ä¿ MUST/JOIN/GROUP/AGG ä¸­æ¶‰åŠåˆ—éƒ½åœ¨ç™½åå•å†…ï¼‰
    def _ensure_contract_columns(plan_obj, selected_map):
        try:
            import re as _re
            def _add_col(tbl, col):
                if tbl in selected_map and col not in selected_map[tbl]:
                    selected_map[tbl].append(col)
            texts = []
            texts += plan_obj.required_predicates or []
            texts += plan_obj.required_joins or []
            texts += plan_obj.groupby or []
            texts += plan_obj.aggregates or []
            for s in texts:
                for t, c in _re.findall(r"\b([a-zA-Z_][a-zA-Z0-9_]*)\.([a-zA-Z_][a-zA-Z0-9_]*)\b", str(s)):
                    if t in selected_map:
                        _add_col(t, c)
        except Exception:
            pass
    _ensure_contract_columns(plan, selected_cols)

    allowed_cols = _build_allowed_columns_string(
        effective_schema,
        table_names_for_kb,
        selected_columns_by_table=selected_cols
    )
    
    # Step 3: æ„å»ºSafety Contract
    print("\nğŸ”’ Step 3: æ„å»ºSafety Contract...")
    
    # åªå…è®¸å­˜åœ¨äºæœ‰æ•ˆ Schema ä¸­çš„è¡¨è¿›å…¥åˆåŒ
    allowed_tables_in_schema = [t.get("name") for t in effective_schema.get("tables", [])]
    safety_contract = make_safety_contract(
        allowed_tables=allowed_tables_in_schema,
        allowed_cols=selected_cols,
        must_tables=getattr(plan, 'must_tables', []) or getattr(plan, 'required_tables', []),
        required_joins=plan.required_joins,
        required_predicates=plan.required_predicates,
        should_predicates=getattr(plan, 'should_predicates', []),
        should_projection=getattr(plan, 'should_projection', []),
        may_predicates=getattr(plan, 'may_predicates', []),
        may_projection=getattr(plan, 'may_projection', []),
        timeframe_days=plan.timeframe_days,
        forbidden_clauses=[] if plan.task == "trend" else ["ORDER BY"]
    )
    
    # Step 4: LLMè°ƒç”¨#2 - Generatorï¼ˆä¸¥æ ¼ä¼ å…¥åˆåŒå†…çš„è¡¨/åˆ—ï¼‰
    print("\nğŸ’¡ Step 4: è°ƒç”¨Generatorç”ŸæˆSQLå€™é€‰...")
    
    try:
        # å…¼å®¹ä¸åŒPydanticç‰ˆæœ¬ï¼Œå®‰å…¨è·å–plançš„JSONå­—ç¬¦ä¸²
        try:
            plan_json_str = plan.model_dump_json(exclude_none=True)
        except Exception:
            try:
                # éƒ¨åˆ†ç¯å¢ƒå¯èƒ½å­˜åœ¨æ—§ç‰ˆAPI
                plan_json_str = plan.json(exclude_none=True)
            except Exception:
                plan_json_str = json.dumps(getattr(plan, "model_dump", lambda **_: plan.dict())(exclude_none=True), ensure_ascii=False)

        candidates = llm_generate_sql(
            question=args.question,
            plan_json=plan_json_str,
            safety_contract=safety_contract,
            n_candidates=max(3, args.sql_topk or 3)
        )
        
        if not candidates:
            print("âš ï¸ Generatoræœªè¿”å›å€™é€‰")
            return do_ask_traditional(args, m_schema, kb_catalog, dialect)
            
        print(f"âœ… ç”Ÿæˆ{len(candidates)}ä¸ªå€™é€‰SQL")
        
    except Exception as e:
        print(f"âŒ Generatorå¤±è´¥: {e}")
        return do_ask_traditional(args, m_schema, kb_catalog, dialect)
    
    # Step 5: ä½¿ç”¨æ–°çš„éªŒè¯å¼•æ“
    print("\nğŸ¯ Step 5: éªŒè¯å’Œé€‰æ‹©æœ€ä½³SQL...")
    
    # ä½¿ç”¨æ–°çš„éªŒè¯å¼•æ“è¿›è¡Œå®¢è§‚éªŒè¯å’Œé€‰æ‹©
    best_candidate = validate_and_select_best(candidates, plan, safety_contract)
    
    if not best_candidate:
        print("âš ï¸ æ‰€æœ‰å€™é€‰éƒ½æœªé€šè¿‡MUSTçº¦æŸéªŒè¯")
        return do_ask_traditional(args, m_schema, kb_catalog, dialect)
    
    best_sql = best_candidate.get("sql", "")
    is_repaired = best_candidate.get("repaired", False)
    
    print(f"âœ… é€‰æ‹©æœ€ä½³å€™é€‰{'ï¼ˆå·²ä¿®å¤ï¼‰' if is_repaired else ''}")
    
    # Step 6: æœ€ç»ˆéªŒè¯å’Œæ”¹å†™
    print("\nğŸ›¡ï¸ Step 6: SQL GuardéªŒè¯...")
    
    try:
        final_sql = validate_and_rewrite(
            best_sql,
            dialect=dialect,
            m_schema=effective_schema,
            max_limit=DEFAULT_MAX_LIMIT,
            keep_order_by=(plan.task == "trend"),
            allowed_columns_by_table=selected_cols
        )
        
        print("\nâœ… æœ€ç»ˆSQL:")
        print(final_sql)
        
        # ä¿å­˜ç»“æœ
        results = [{
            "question": args.question,
            "sql": final_sql,
            "method": "two_call_clean",
            "repaired": is_repaired,
            "plan": plan.model_dump() if plan else None
        }]
        
        # æ·»åŠ å…¶ä»–å€™é€‰ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.sql_topk and args.sql_topk > 1:
            for candidate in candidates[1:args.sql_topk]:
                try:
                    sql = candidate.get("sql", "")
                    if sql:
                        validated_sql = validate_and_rewrite(
                            sql,
                            dialect=dialect,
                            m_schema=effective_schema,
                            max_limit=DEFAULT_MAX_LIMIT,
                            keep_order_by=(plan.task == "trend"),
                            allowed_columns_by_table=selected_cols
                        )
                        results.append({
                            "question": args.question,
                            "sql": validated_sql,
                            "method": "additional_candidate"
                        })
                except:
                    pass
        
        # è¾“å‡ºç»“æœ
        if args.output:
            with open(args.output, "w", encoding="utf-8") as f:
                for r in results:
                    f.write(json.dumps(r, ensure_ascii=False) + "\n")
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        
        return results
        
    except SQLValidationError as e:
        print(f"âŒ SQLéªŒè¯å¤±è´¥: {e}")
        return do_ask_traditional(args, m_schema, kb_catalog, dialect)


def do_ask_traditional(args, m_schema, kb_catalog, dialect):
    """ä¼ ç»Ÿæ¨¡å¼å›é€€ï¼ˆå•æ¬¡è°ƒç”¨ï¼‰"""
    print("\nâš ï¸ å›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼...")
    
    # ç®€å•çš„è¡¨å’Œåˆ—é€‰æ‹©
    tokens = tokenize(args.question)
    effective_schema, scored_tables = auto_select_tables(m_schema, tokens, topk=6)
    table_names = [name for name, _ in scored_tables]
    
    if not table_names:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³è¡¨")
        return []
    
    # åˆ—é€‰æ‹©
    selected_cols = _select_columns_simple(effective_schema, table_names, tokens)
    
    # æ„å»ºå…è®¸åˆ—
    allowed_cols = _build_allowed_columns_string(
        effective_schema, table_names,
        selected_columns_by_table=selected_cols
    )
    
    # è°ƒç”¨ä¼ ç»Ÿç”Ÿæˆ
    try:
        sql_result = call_xiyan(
            question=args.question,
            m_schema=effective_schema,
            evidence="",
            kb_snippet="",
            allowed_columns=allowed_cols
        )
        sql_list = [sql_result] if sql_result else []
        
        if not sql_list:
            print("âŒ ä¼ ç»Ÿç”Ÿæˆå¤±è´¥")
            return []
        
        # éªŒè¯å’Œè¾“å‡º
        results = []
        for i, sql in enumerate(sql_list):
            try:
                final_sql = validate_and_rewrite(
                    sql, dialect=dialect,
                    m_schema=effective_schema,
                    max_limit=DEFAULT_MAX_LIMIT
                )
                results.append({
                    "question": args.question,
                    "sql": final_sql,
                    "method": "traditional"
                })
                
                if i == 0:
                    print(f"\nâœ… æœ€ç»ˆSQL:\n{final_sql}")
                    
            except Exception as e:
                print(f"  SQL{i+1}éªŒè¯å¤±è´¥: {e}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ä¼ ç»Ÿç”Ÿæˆå¼‚å¸¸: {e}")
        return []


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Text-to-SQL Clean Version")
    subparsers = parser.add_subparsers(dest="command", help="å‘½ä»¤")
    
    # askå‘½ä»¤
    ask_parser = subparsers.add_parser("ask", help="ç”ŸæˆSQL")
    ask_parser.add_argument("--question", "-q", required=True, help="è‡ªç„¶è¯­è¨€é—®é¢˜")
    ask_parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    ask_parser.add_argument("--sql-topk", type=int, default=1, help="ç”ŸæˆSQLæ•°é‡")
    ask_parser.add_argument("--use-semantic", action="store_true", help="ä½¿ç”¨è¯­ä¹‰æ£€ç´¢")
    ask_parser.add_argument("--best", action="store_true", help="ä½¿ç”¨æœ€ä½³é…ç½®")
    
    args = parser.parse_args()
    
    if args.command == "ask":
        # --bestè‡ªåŠ¨å¼€å¯æ‰€æœ‰ä¼˜åŒ–
        if args.best:
            args.use_semantic = True
            args.sql_topk = max(3, args.sql_topk)
        
        results = do_ask(args)
        
        if not results:
            print("\nâŒ ç”Ÿæˆå¤±è´¥")
            sys.exit(1)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
